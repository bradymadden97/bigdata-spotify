{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Final Project: Spotify Song Analysis\n",
    "### Beske, Devico, Madden, & Stone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The following code block contains necessary imports for the project...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import requests\n",
    "import json\n",
    "import urllib2\n",
    "import csv\n",
    "\n",
    "# Custom imports\n",
    "#import spotify_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import sys\n",
    "import re\n",
    "\n",
    "class Distribution(dict):\n",
    "\t\"\"\"a very simple class which represents a probability distribution\n",
    "\t   subclasses Dict, so can be used easily.\n",
    "\n",
    "\t   The count field is used by corpus to keep track of type counts.\"\"\"\n",
    "\tdef __init__(self, count=0.0, *dict_args):\n",
    "\t\tdict.__init__(self, *dict_args)\n",
    "\t\tself.count = count\n",
    "\n",
    "\t\t# include unknown token w/ count of 0\n",
    "\t\tself['<unk>'] = 0.0\n",
    "\n",
    "\n",
    "\tdef add(self, key):\n",
    "\t\tif key not in self:\n",
    "\t\t\tself[key] = 1.0\n",
    "\t\telse:\n",
    "\t\t\tself[key] += 1\n",
    "\n",
    "\tdef add(self, key, k):\n",
    "\t\tif key not in self:\n",
    "\t\t\tself[key] = 1.0 + k\n",
    "\t\telse:\n",
    "\t\t\tself[key] += 1\n",
    "\n",
    "\n",
    "\tdef total_counts(self):\n",
    "\t\treturn sum(self.values())\n",
    "\n",
    "\tdef __iadd__(self, other):\n",
    "\t\tself.count += other\n",
    "\t\treturn self\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_dict(cls, d):\n",
    "\t\t\"\"\" create a Distribution from a dict \"\"\"\n",
    "\t\tfor k,v in d.items():\n",
    "\t\t\tassert type(v) == float, \"Values in d must be ints\"\n",
    "\t\t\tassert type(k) == str, \"Keys in d must be strings\"\n",
    "\n",
    "\t\tdist = Distribution()\n",
    "\t\tfor key, value in d.items():\n",
    "\t\t\tdist[key] = value\n",
    "\t\treturn dist\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "\t\"\"\"a class which represents unigrams and bigrams extracted from a corpus.\n",
    "\t   Consists of a dictionary whose keys are types, and whose values are\n",
    "\t   a Distribution of following (subsequent) types.\n",
    "\t   \"\"\"\n",
    "\tdef __init__(self, k = None):\n",
    "\t\t\"\"\" returns an empty corpus \"\"\"\n",
    "\t\tself._types = {'<unk>': Distribution()}\n",
    "\t\tself.vocab_size = 0.0\n",
    "\t\tself.token_num = 0.0\n",
    "\t\tif k == None:\n",
    "\t\t\tself.k = 0.0055\n",
    "\t\telse:\n",
    "\t\t\tself.k = k\n",
    "\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t\"\"\" return the length of the vocabulary \"\"\"\n",
    "\t\treturn len(self._types)\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\t\"\"\" pretty-printable representation \"\"\"\n",
    "\t\ttitle = \"Corpus with {} unique words:\".format(len(self))\n",
    "\t\ttitle += ('\\n' + len(title)*'-' + '\\n')\n",
    "\n",
    "\t\tfor word, dist in self._types.items():\n",
    "\t\t\ttitle += '{:<20}{:>4}|{}\\n'.format(word, dist.count, dist)\n",
    "\t\treturn title\n",
    "\n",
    "\tdef unigram(self):\n",
    "\t\treturn { word : dist.count for word, dist in self._types.items()}\n",
    "\n",
    "\tdef bigram(self, token):\n",
    "\t\t\"\"\" returns the <unk> distribution if we haven't seen this token \"\"\"\n",
    "\t\ttoken = token if token in self._types else '<unk>'\n",
    "\t\treturn copy.deepcopy(self._types[token])\n",
    "\n",
    "\tdef add_unigram(self, token):\n",
    "\t\t''' returns True if token is new, False otherwise '''\n",
    "\t\tself.token_num += 1\n",
    "\n",
    "\t\tif token not in self._types:\n",
    "\t\t\t# create an entry in self._types so we know we've seen it for next time\n",
    "\t\t\tself._types[token] = Distribution()\n",
    "\t\t\tself._types[token].count += 1.0 # increment count of token\n",
    "\t\t\tself.vocab_size += 1\n",
    "\t\t\treturn False\n",
    "\t\telif self._types[token].count > 1.0:\n",
    "\t\t\tself._types[token].count += 1.0\n",
    "\t\t\treturn False\n",
    "\t\telif self._types[token].count == 1.0:\n",
    "\t\t\t#treat as unknown if it is second occurence of token\n",
    "\t\t\tself._types['<unk>'].count += 1.0 # increment count of <unk>\n",
    "\t\t\tself._types[token].count += 0.000000000000001\n",
    "\t\t\treturn True\n",
    "\n",
    "\n",
    "\tdef add_bigram(self, token1, token2):\n",
    "\t\t''' add a bigram to the corpus.\n",
    "\t\t\ttoken1 is the first token in the bigram\n",
    "\t\t\ttoken2 is the second token '''\n",
    "\t\t# we should add token2 to <unk> if token1 is new, otherwise we can\n",
    "\t\t# add it to token1\n",
    "\n",
    "\t\tadd_to = '<unk>' if self.add_unigram(token1) else token1\n",
    "\n",
    "\t\tif token2 in self._types and self._types[token2].count == 1.0:\n",
    "\t\t\tself._types[add_to].add('<unk>', self.k)\n",
    "\t\telse:\n",
    "\t\t\tself._types[add_to].add(token2, self.k)\n",
    "\t\treturn\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_file(cls, file_path, k = None):\n",
    "\t\t''' create a Corpus from a file '''\n",
    "\t\tif k == None:\n",
    "\t\t\tcorpus = Corpus()\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(k)\n",
    "\t\twith open(file_path, 'r') as f:\n",
    "\t\t\ttokens = f.read().split() # spliting on any whitespace\n",
    "\n",
    "\t\t\t# subtract one so we skip the last token, which isn't a bigram\n",
    "\t\t\t# iterate over tokens with window of size 2\n",
    "\t\t\tfor idx in range(len(tokens)-1): corpus.add_bigram(tokens[idx], tokens[idx+1])\n",
    "\t\treturn corpus\n",
    "\n",
    "\t@classmethod\n",
    "\tdef from_string(cls, s, k = None):\n",
    "\t\tif k == None:\n",
    "\t\t\tcorpus = Corpus()\n",
    "\t\telse:\n",
    "\t\t\tcorpus = Corpus(k)\n",
    "\n",
    "\t\ttokens = re.findall(r'\\S+|\\n',s) # spliting on any whitespace\n",
    "\t\tfor idx in range(len(tokens)-1): corpus.add_bigram(tokens[idx], tokens[idx+1])\n",
    "\t\treturn corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Generator(object): \n",
    "    \n",
    "    # Class attribute\n",
    "    punctuation = [\".\", \",\", \";\", \":\", \"?\", \"...\", \"!\"]\n",
    "    special = [\"<unk>\"]\n",
    "    \n",
    "    def __init__(self, s, line):\n",
    "        self.end_tokens = [\".\", \"!\", \"?\", \"<END>\"]\n",
    "        self.cps = Corpus.from_string(s)\n",
    "        self.ug = self.cps.unigram()\n",
    "        if line:\n",
    "            self.end_tokens += \"\\n\"\n",
    "    \n",
    "    def select_word(self, dist, ignore, n):\n",
    "        \"\"\" Selects a word from a given distribution using its weight\n",
    "        Arguments: \n",
    "        dist: the distribution as a dictionary\n",
    "        ignore (optional): list of tokens to be ignored, leave empty if unused\n",
    "        n: the sum of all non-ignored counts in the distribution, require n>0\n",
    "        \"\"\"\n",
    "        n = int(n)\n",
    "        \n",
    "        if n < 1:\n",
    "            # Should fail here\n",
    "            pass\n",
    "        \n",
    "        word = '<unk>'\n",
    "        r = random.randint(1, n)\n",
    "        for key in dist:\n",
    "            if (not key in ignore):\n",
    "                r -= dist[key]\n",
    "                if r <= 0:\n",
    "                    return key\n",
    "\n",
    "        return word\n",
    "    \n",
    "    def generate(self, n, mode, ignore, sentences):\n",
    "        \"\"\" Arguments:\n",
    "        n: The number of sentences to generate\n",
    "        mode: either 'u' or 'b' for unigram or bigram based generation\n",
    "        ignore (optional): a list of tokens to ignore\n",
    "        sentences (optional): a list of sentence stems (e.g. [\"The world was\"]),\n",
    "        if present, the generator will complete the sentence stems instead of\n",
    "        generating new sentences. If the stems are the empty string or the list\n",
    "        is empty, up to n completely new sentences will be generated instead\n",
    "        Returns an array of randomly generated sentences \n",
    "        \"\"\"\n",
    "        if n <= 0 or (mode != 'u' and mode != 'b'):\n",
    "            return []\n",
    "        \n",
    "        for i in range(n):\n",
    "            # Use the last word in the sentence stem to proceed with generation\n",
    "            if len(sentences) > i and sentences[i] != \"\":\n",
    "                word = sentences[i].split()[-1]\n",
    "            # Or get the first word from the unigram distribution if no sentence stems available\n",
    "            else:\n",
    "                ugTotal = 0\n",
    "                for key in self.ug:\n",
    "                    if not key in Generator.punctuation + ignore:    \n",
    "                        ugTotal += self.ug[key]\n",
    "                word = self.select_word(self.ug, Generator.punctuation + ignore, ugTotal)\n",
    "                if len(sentences) <= (i+1):\n",
    "                    sentences.append(word)\n",
    "                else:\n",
    "                    sentences[i] = word\n",
    "            \n",
    "            # generate a sentence, using the end_tokens for termination\n",
    "            while not word in self.end_tokens:\n",
    "                \n",
    "                if mode == 'u':\n",
    "                    #  Unigram generation\n",
    "                    word = self.select_word(self.ug, ignore, ugTotal)\n",
    "                else:  \n",
    "                    # Get the bigram distribution and counts\n",
    "                    bg = self.cps.bigram(word)\n",
    "                    total = 0\n",
    "                    for key in bg:\n",
    "                        if not key in ignore:\n",
    "                            total += bg[key]\n",
    "                    \n",
    "                    # Terminate the sentence if there are no non-ignored tokens in the bigram\n",
    "                    if total == 0:\n",
    "                        sentences[i] += \" . \"\n",
    "                        break\n",
    "                \n",
    "                    word = self.select_word(bg, ignore, total)\n",
    "                \n",
    "                sentences[i] += \" \" + word\n",
    "        \n",
    "        return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "with open('data/lyrics_big.csv', 'rb') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    training_data = {}\n",
    "    test_data = {}\n",
    "    genres = {}\n",
    "    i = 0\n",
    "    for row in reader:\n",
    "        if row['genre'] == 'Not Available' or row['genre'] == 'Other':\n",
    "            pass\n",
    "        elif row['genre'] in genres:\n",
    "            genres[row['genre']] += \" <END> \"\n",
    "            genres[row['genre']] += row['lyrics']\n",
    "        else:\n",
    "            genres[row['genre']] = row['lyrics']\n",
    "#         if (i % 10 == 0):\n",
    "#             training_data[row['text']] = row['artist']\n",
    "#         else:\n",
    "#             test_data[row['text']] = row['artist']\n",
    "        i += 1\n",
    "        if (i == 100000):\n",
    "            break\n",
    "        elif (i % 10000 == 0):\n",
    "            print \"...\"\n",
    "print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Indie', 'Country', 'Jazz', 'Metal', 'Pop', 'R&B', 'Folk', 'Rock', 'Electronic', 'Hip-Hop']\n",
      "13735997\n",
      "812093\n",
      "3859442\n",
      "1954518\n",
      "5941030\n",
      "13735997\n",
      "827472\n",
      "650686\n",
      "28705005\n",
      "2150468\n",
      "16088863\n"
     ]
    }
   ],
   "source": [
    "print genres.keys()\n",
    "print len(genres['Pop'])\n",
    "for key in genres.keys():\n",
    "    print len(genres[key])\n",
    "    \n",
    "#print genres[\"Pop\"][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pop_done\n",
      "hip-hop done\n",
      "country done\n",
      "metal done\n"
     ]
    }
   ],
   "source": [
    "ignore = [\"\\xc3\",\"\\x8e\",\"\\xc2\",\"\\xa4\",\"\\xad\",\"\\x8f\",\"\\x81\",\"\\xbc\",\"\\xb1\",\"\\xb7\",\"\\xb3\",\"\\xac\",\"\\x80\",\"\\xbd\",\"\\xb6\",\"\\xb5\",\"\\xb9\",\"\\x82\"]\n",
    "\n",
    "pop = genres['Pop']\n",
    "pop_gen_lines = Generator(pop, True)\n",
    "pop_gen_songs = Generator(pop, False)\n",
    "\n",
    "print \"Pop_done\"\n",
    "\n",
    "hiphop = genres['Hip-Hop']\n",
    "hiphop_gen_lines = Generator(hiphop, True)\n",
    "hiphop_gen_songs = Generator(hiphop, False)\n",
    "\n",
    "print 'hip-hop done'\n",
    "\n",
    "country = genres['Country']\n",
    "country_gen_lines = Generator(country, True)\n",
    "country_gen_songs = Generator(country, False)\n",
    "\n",
    "print 'country done'\n",
    "\n",
    "metal = genres['Metal']\n",
    "metal_gen_lines = Generator(metal, True)\n",
    "metal_gen_songs = Generator(metal, False)\n",
    "\n",
    "print 'metal done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my wife but this MF rookie (yup) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pop_line = hiphop_gen_lines.generate(1, 'b', Generator.special + ignore, [])\n",
    "print pop_line[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
